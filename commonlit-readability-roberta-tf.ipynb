{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hispanic-franklin",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-06-10T05:48:29.102156Z",
     "iopub.status.busy": "2021-06-10T05:48:29.100677Z",
     "iopub.status.idle": "2021-06-10T05:48:36.149224Z",
     "shell.execute_reply": "2021-06-10T05:48:36.148667Z",
     "shell.execute_reply.started": "2021-06-07T10:13:59.461399Z"
    },
    "papermill": {
     "duration": 7.062049,
     "end_time": "2021-06-10T05:48:36.149376",
     "exception": false,
     "start_time": "2021-06-10T05:48:29.087327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras import backend as K\n",
    "from transformers import RobertaTokenizer, TFRobertaModel\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ultimate-distributor",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T05:48:36.170722Z",
     "iopub.status.busy": "2021-06-10T05:48:36.170199Z",
     "iopub.status.idle": "2021-06-10T05:48:36.279598Z",
     "shell.execute_reply": "2021-06-10T05:48:36.279136Z",
     "shell.execute_reply.started": "2021-06-07T10:25:48.352942Z"
    },
    "papermill": {
     "duration": 0.121779,
     "end_time": "2021-06-10T05:48:36.279731",
     "exception": false,
     "start_time": "2021-06-10T05:48:36.157952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configurations\n",
    "EPOCHS = 50\n",
    "# Batch size\n",
    "BATCH_SIZE = 24\n",
    "# Seed\n",
    "SEED = 123\n",
    "# Learning rate\n",
    "LR = 0.000040\n",
    "# Verbosity\n",
    "VERBOSE = 1\n",
    "# Number of folds for training\n",
    "FOLDS = 5\n",
    "\n",
    "# Max length\n",
    "MAX_LEN = 250\n",
    "\n",
    "ES_PATIENCE = 7\n",
    "# PATIENCE = 2\n",
    "\n",
    "# Get the trained model we want to use\n",
    "MODEL = '../input/huggingface-roberta/roberta-base'\n",
    "\n",
    "# Let's load our model tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# For tf.dataset\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "nasty-policy",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T05:48:36.303170Z",
     "iopub.status.busy": "2021-06-10T05:48:36.302417Z",
     "iopub.status.idle": "2021-06-10T05:48:36.304684Z",
     "shell.execute_reply": "2021-06-10T05:48:36.305043Z",
     "shell.execute_reply.started": "2021-06-07T10:14:06.415566Z"
    },
    "papermill": {
     "duration": 0.017111,
     "end_time": "2021-06-10T05:48:36.305165",
     "exception": false,
     "start_time": "2021-06-10T05:48:36.288054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    excerpt_processed = []\n",
    "    for i in tqdm(data['excerpt']):\n",
    "        \n",
    "        i = re.sub(\"[^a-zA-Z]\",\" \",i)\n",
    "        i = i.lower()\n",
    "        i = nltk.word_tokenize(i)\n",
    "        i = [word for word in i if not word in set(stopwords.words(\"english\"))]\n",
    "        \n",
    "        lemma = nltk.WordNetLemmatizer()\n",
    "        i = [lemma.lemmatize(word) for word in i]\n",
    "        i=\" \".join(i)\n",
    "        excerpt_processed.append(i)\n",
    "    return excerpt_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dying-aircraft",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T05:48:36.325893Z",
     "iopub.status.busy": "2021-06-10T05:48:36.325169Z",
     "iopub.status.idle": "2021-06-10T05:48:36.327794Z",
     "shell.execute_reply": "2021-06-10T05:48:36.327385Z",
     "shell.execute_reply.started": "2021-06-07T10:14:06.426201Z"
    },
    "papermill": {
     "duration": 0.014646,
     "end_time": "2021-06-10T05:48:36.327897",
     "exception": false,
     "start_time": "2021-06-10T05:48:36.313251",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to seed everything\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "nervous-coating",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T05:48:36.348243Z",
     "iopub.status.busy": "2021-06-10T05:48:36.347521Z",
     "iopub.status.idle": "2021-06-10T05:48:36.350198Z",
     "shell.execute_reply": "2021-06-10T05:48:36.349807Z",
     "shell.execute_reply.started": "2021-06-07T10:14:06.434983Z"
    },
    "papermill": {
     "duration": 0.014344,
     "end_time": "2021-06-10T05:48:36.350297",
     "exception": false,
     "start_time": "2021-06-10T05:48:36.335953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function tokenize the text according to a transformers model tokenizer\n",
    "def regular_encode(text, tokenizer, max_len=MAX_LEN):\n",
    "    encode_dict = tokenizer.batch_encode_plus(\n",
    "                text,\n",
    "                padding = 'max_length',\n",
    "                truncation = True,\n",
    "                max_length = max_len)\n",
    "    return np.array(encode_dict['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "major-better",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T05:48:36.370926Z",
     "iopub.status.busy": "2021-06-10T05:48:36.370241Z",
     "iopub.status.idle": "2021-06-10T05:48:36.372582Z",
     "shell.execute_reply": "2021-06-10T05:48:36.373016Z",
     "shell.execute_reply.started": "2021-06-07T10:14:06.445554Z"
    },
    "papermill": {
     "duration": 0.014801,
     "end_time": "2021-06-10T05:48:36.373129",
     "exception": false,
     "start_time": "2021-06-10T05:48:36.358328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function encode our training sentences\n",
    "def encode_texts(x_train, x_val, MAX_LEN):\n",
    "    x_train = regular_encode(x_train.tolist(), tokenizer, max_len = MAX_LEN)\n",
    "    x_val = regular_encode(x_val.tolist(), tokenizer, max_len = MAX_LEN)\n",
    "    return x_train, x_val\n",
    "\n",
    "def encode_texts_test(x_test, MAX_LEN):\n",
    "    x_test = regular_encode(x_test.tolist(), tokenizer, max_len = MAX_LEN)\n",
    "    return x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "infinite-church",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T05:48:36.394963Z",
     "iopub.status.busy": "2021-06-10T05:48:36.394260Z",
     "iopub.status.idle": "2021-06-10T05:48:36.396943Z",
     "shell.execute_reply": "2021-06-10T05:48:36.396493Z",
     "shell.execute_reply.started": "2021-06-07T10:14:06.458927Z"
    },
    "papermill": {
     "duration": 0.015832,
     "end_time": "2021-06-10T05:48:36.397042",
     "exception": false,
     "start_time": "2021-06-10T05:48:36.381210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to transform arrays to tensors\n",
    "def transform_to_tensors(x_train, x_val, y_train, y_val):\n",
    "    train_dataset = (\n",
    "        tf.data.Dataset\n",
    "        .from_tensor_slices((x_train, y_train))\n",
    "        .repeat()\n",
    "        .shuffle(2048)\n",
    "        .batch(BATCH_SIZE)\n",
    "        .prefetch(AUTO)\n",
    "    )\n",
    "\n",
    "    valid_dataset = (\n",
    "        tf.data.Dataset\n",
    "        .from_tensor_slices((x_val, y_val))\n",
    "        .batch(BATCH_SIZE)\n",
    "        .prefetch(AUTO)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return train_dataset, valid_dataset\n",
    "\n",
    "def transform_to_tensors_test(x_test):\n",
    "    test_dataset = (\n",
    "        tf.data.Dataset\n",
    "        .from_tensor_slices((x_test))\n",
    "        .batch(BATCH_SIZE)\n",
    "        .prefetch(AUTO)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "liberal-absolute",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T05:48:36.430211Z",
     "iopub.status.busy": "2021-06-10T05:48:36.429655Z",
     "iopub.status.idle": "2021-06-10T07:04:41.815627Z",
     "shell.execute_reply": "2021-06-10T07:04:41.815135Z",
     "shell.execute_reply.started": "2021-06-10T05:47:51.136793Z"
    },
    "papermill": {
     "duration": 4565.410597,
     "end_time": "2021-06-10T07:04:41.815763",
     "exception": false,
     "start_time": "2021-06-10T05:48:36.405166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../input/huggingface-roberta/roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/huggingface-roberta/roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "94/94 [==============================] - 90s 810ms/step - loss: 0.8645 - root_mean_squared_error: 0.9231 - val_loss: 0.5445 - val_root_mean_squared_error: 0.7379\n",
      "\n",
      "Epoch 00001: val_root_mean_squared_error improved from inf to 0.73794, saving model to Roberta_Base_123_1.h5\n",
      "Epoch 2/50\n",
      "94/94 [==============================] - 73s 781ms/step - loss: 0.3439 - root_mean_squared_error: 0.5850 - val_loss: 0.4657 - val_root_mean_squared_error: 0.6825\n",
      "\n",
      "Epoch 00002: val_root_mean_squared_error improved from 0.73794 to 0.68246, saving model to Roberta_Base_123_1.h5\n",
      "Epoch 3/50\n",
      "94/94 [==============================] - 73s 780ms/step - loss: 0.2338 - root_mean_squared_error: 0.4831 - val_loss: 0.3326 - val_root_mean_squared_error: 0.5767\n",
      "\n",
      "Epoch 00003: val_root_mean_squared_error improved from 0.68246 to 0.57667, saving model to Roberta_Base_123_1.h5\n",
      "Epoch 4/50\n",
      "94/94 [==============================] - 73s 781ms/step - loss: 0.1547 - root_mean_squared_error: 0.3930 - val_loss: 0.4562 - val_root_mean_squared_error: 0.6755\n",
      "\n",
      "Epoch 00004: val_root_mean_squared_error did not improve from 0.57667\n",
      "Epoch 5/50\n",
      "94/94 [==============================] - 73s 781ms/step - loss: 0.1068 - root_mean_squared_error: 0.3266 - val_loss: 0.3467 - val_root_mean_squared_error: 0.5888\n",
      "\n",
      "Epoch 00005: val_root_mean_squared_error did not improve from 0.57667\n",
      "Epoch 6/50\n",
      "94/94 [==============================] - 73s 780ms/step - loss: 0.0807 - root_mean_squared_error: 0.2840 - val_loss: 0.2927 - val_root_mean_squared_error: 0.5410\n",
      "\n",
      "Epoch 00006: val_root_mean_squared_error improved from 0.57667 to 0.54098, saving model to Roberta_Base_123_1.h5\n",
      "Epoch 7/50\n",
      "94/94 [==============================] - 73s 782ms/step - loss: 0.1478 - root_mean_squared_error: 0.3831 - val_loss: 0.5206 - val_root_mean_squared_error: 0.7215\n",
      "\n",
      "Epoch 00007: val_root_mean_squared_error did not improve from 0.54098\n",
      "Epoch 8/50\n",
      "94/94 [==============================] - 73s 781ms/step - loss: 0.0938 - root_mean_squared_error: 0.3057 - val_loss: 0.5513 - val_root_mean_squared_error: 0.7425\n",
      "\n",
      "Epoch 00008: val_root_mean_squared_error did not improve from 0.54098\n",
      "Epoch 9/50\n",
      "94/94 [==============================] - 73s 780ms/step - loss: 0.0617 - root_mean_squared_error: 0.2475 - val_loss: 0.3766 - val_root_mean_squared_error: 0.6137\n",
      "\n",
      "Epoch 00009: val_root_mean_squared_error did not improve from 0.54098\n",
      "Epoch 10/50\n",
      "94/94 [==============================] - 73s 781ms/step - loss: 0.0388 - root_mean_squared_error: 0.1968 - val_loss: 0.3989 - val_root_mean_squared_error: 0.6316\n",
      "\n",
      "Epoch 00010: val_root_mean_squared_error did not improve from 0.54098\n",
      "Epoch 11/50\n",
      "94/94 [==============================] - 73s 781ms/step - loss: 0.0303 - root_mean_squared_error: 0.1738 - val_loss: 0.4132 - val_root_mean_squared_error: 0.6428\n",
      "\n",
      "Epoch 00011: val_root_mean_squared_error did not improve from 0.54098\n",
      "Epoch 12/50\n",
      "94/94 [==============================] - 73s 780ms/step - loss: 0.0314 - root_mean_squared_error: 0.1769 - val_loss: 0.3211 - val_root_mean_squared_error: 0.5666\n",
      "\n",
      "Epoch 00012: val_root_mean_squared_error did not improve from 0.54098\n",
      "Epoch 13/50\n",
      "94/94 [==============================] - 73s 780ms/step - loss: 0.0354 - root_mean_squared_error: 0.1876 - val_loss: 0.4343 - val_root_mean_squared_error: 0.6590\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00013: val_root_mean_squared_error did not improve from 0.54098\n",
      "Epoch 00013: early stopping\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../input/huggingface-roberta/roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/huggingface-roberta/roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "94/94 [==============================] - 90s 812ms/step - loss: 0.8072 - root_mean_squared_error: 0.8895 - val_loss: 0.6830 - val_root_mean_squared_error: 0.8265\n",
      "\n",
      "Epoch 00001: val_root_mean_squared_error improved from inf to 0.82647, saving model to Roberta_Base_123_2.h5\n",
      "Epoch 2/50\n",
      "94/94 [==============================] - 73s 782ms/step - loss: 0.3183 - root_mean_squared_error: 0.5619 - val_loss: 0.3743 - val_root_mean_squared_error: 0.6118\n",
      "\n",
      "Epoch 00002: val_root_mean_squared_error improved from 0.82647 to 0.61180, saving model to Roberta_Base_123_2.h5\n",
      "Epoch 3/50\n",
      "94/94 [==============================] - 73s 781ms/step - loss: 0.2033 - root_mean_squared_error: 0.4508 - val_loss: 0.5804 - val_root_mean_squared_error: 0.7619\n",
      "\n",
      "Epoch 00003: val_root_mean_squared_error did not improve from 0.61180\n",
      "Epoch 4/50\n",
      "94/94 [==============================] - 73s 780ms/step - loss: 0.1486 - root_mean_squared_error: 0.3849 - val_loss: 0.3569 - val_root_mean_squared_error: 0.5975\n",
      "\n",
      "Epoch 00004: val_root_mean_squared_error improved from 0.61180 to 0.59745, saving model to Roberta_Base_123_2.h5\n",
      "Epoch 5/50\n",
      "94/94 [==============================] - 73s 780ms/step - loss: 0.1108 - root_mean_squared_error: 0.3324 - val_loss: 0.3083 - val_root_mean_squared_error: 0.5553\n",
      "\n",
      "Epoch 00005: val_root_mean_squared_error improved from 0.59745 to 0.55529, saving model to Roberta_Base_123_2.h5\n",
      "Epoch 6/50\n",
      "94/94 [==============================] - 73s 780ms/step - loss: 0.1069 - root_mean_squared_error: 0.3266 - val_loss: 0.3907 - val_root_mean_squared_error: 0.6251\n",
      "\n",
      "Epoch 00006: val_root_mean_squared_error did not improve from 0.55529\n",
      "Epoch 7/50\n",
      "94/94 [==============================] - 73s 781ms/step - loss: 0.0660 - root_mean_squared_error: 0.2567 - val_loss: 0.3351 - val_root_mean_squared_error: 0.5789\n",
      "\n",
      "Epoch 00007: val_root_mean_squared_error did not improve from 0.55529\n",
      "Epoch 8/50\n",
      "94/94 [==============================] - 73s 780ms/step - loss: 0.0576 - root_mean_squared_error: 0.2399 - val_loss: 0.4122 - val_root_mean_squared_error: 0.6420\n",
      "\n",
      "Epoch 00008: val_root_mean_squared_error did not improve from 0.55529\n",
      "Epoch 9/50\n",
      "94/94 [==============================] - 73s 781ms/step - loss: 0.0406 - root_mean_squared_error: 0.2012 - val_loss: 0.3108 - val_root_mean_squared_error: 0.5575\n",
      "\n",
      "Epoch 00009: val_root_mean_squared_error did not improve from 0.55529\n",
      "Epoch 10/50\n",
      "94/94 [==============================] - 73s 781ms/step - loss: 0.0390 - root_mean_squared_error: 0.1972 - val_loss: 0.3399 - val_root_mean_squared_error: 0.5830\n",
      "\n",
      "Epoch 00010: val_root_mean_squared_error did not improve from 0.55529\n",
      "Epoch 11/50\n",
      "94/94 [==============================] - 73s 780ms/step - loss: 0.0314 - root_mean_squared_error: 0.1771 - val_loss: 0.3526 - val_root_mean_squared_error: 0.5938\n",
      "\n",
      "Epoch 00011: val_root_mean_squared_error did not improve from 0.55529\n",
      "Epoch 12/50\n",
      "94/94 [==============================] - 73s 781ms/step - loss: 0.0320 - root_mean_squared_error: 0.1787 - val_loss: 0.3536 - val_root_mean_squared_error: 0.5946\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00012: val_root_mean_squared_error did not improve from 0.55529\n",
      "Epoch 00012: early stopping\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../input/huggingface-roberta/roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/huggingface-roberta/roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "94/94 [==============================] - 90s 812ms/step - loss: 0.8387 - root_mean_squared_error: 0.9039 - val_loss: 0.2917 - val_root_mean_squared_error: 0.5401\n",
      "\n",
      "Epoch 00001: val_root_mean_squared_error improved from inf to 0.54007, saving model to Roberta_Base_123_3.h5\n",
      "Epoch 2/50\n",
      "94/94 [==============================] - 73s 782ms/step - loss: 0.2991 - root_mean_squared_error: 0.5458 - val_loss: 0.3366 - val_root_mean_squared_error: 0.5802\n",
      "\n",
      "Epoch 00002: val_root_mean_squared_error did not improve from 0.54007\n",
      "Epoch 3/50\n",
      "94/94 [==============================] - 73s 782ms/step - loss: 0.1940 - root_mean_squared_error: 0.4404 - val_loss: 0.4797 - val_root_mean_squared_error: 0.6926\n",
      "\n",
      "Epoch 00003: val_root_mean_squared_error did not improve from 0.54007\n",
      "Epoch 4/50\n",
      "94/94 [==============================] - 73s 781ms/step - loss: 0.1783 - root_mean_squared_error: 0.4220 - val_loss: 0.3862 - val_root_mean_squared_error: 0.6214\n",
      "\n",
      "Epoch 00004: val_root_mean_squared_error did not improve from 0.54007\n",
      "Epoch 5/50\n",
      "94/94 [==============================] - 74s 783ms/step - loss: 0.1114 - root_mean_squared_error: 0.3337 - val_loss: 0.6190 - val_root_mean_squared_error: 0.7868\n",
      "\n",
      "Epoch 00005: val_root_mean_squared_error did not improve from 0.54007\n",
      "Epoch 6/50\n",
      "94/94 [==============================] - 73s 780ms/step - loss: 0.1025 - root_mean_squared_error: 0.3190 - val_loss: 0.3617 - val_root_mean_squared_error: 0.6015\n",
      "\n",
      "Epoch 00006: val_root_mean_squared_error did not improve from 0.54007\n",
      "Epoch 7/50\n",
      "94/94 [==============================] - 73s 781ms/step - loss: 0.0598 - root_mean_squared_error: 0.2444 - val_loss: 0.3611 - val_root_mean_squared_error: 0.6009\n",
      "\n",
      "Epoch 00007: val_root_mean_squared_error did not improve from 0.54007\n",
      "Epoch 8/50\n",
      "94/94 [==============================] - 73s 781ms/step - loss: 0.0537 - root_mean_squared_error: 0.2316 - val_loss: 0.3203 - val_root_mean_squared_error: 0.5659\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00008: val_root_mean_squared_error did not improve from 0.54007\n",
      "Epoch 00008: early stopping\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../input/huggingface-roberta/roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/huggingface-roberta/roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "94/94 [==============================] - 90s 807ms/step - loss: 0.9540 - root_mean_squared_error: 0.9689 - val_loss: 0.3456 - val_root_mean_squared_error: 0.5879\n",
      "\n",
      "Epoch 00001: val_root_mean_squared_error improved from inf to 0.58787, saving model to Roberta_Base_123_4.h5\n",
      "Epoch 2/50\n",
      "94/94 [==============================] - 73s 782ms/step - loss: 0.3868 - root_mean_squared_error: 0.6204 - val_loss: 0.6570 - val_root_mean_squared_error: 0.8105\n",
      "\n",
      "Epoch 00002: val_root_mean_squared_error did not improve from 0.58787\n",
      "Epoch 3/50\n",
      "94/94 [==============================] - 73s 781ms/step - loss: 0.2596 - root_mean_squared_error: 0.5090 - val_loss: 0.2567 - val_root_mean_squared_error: 0.5066\n",
      "\n",
      "Epoch 00003: val_root_mean_squared_error improved from 0.58787 to 0.50663, saving model to Roberta_Base_123_4.h5\n",
      "Epoch 4/50\n",
      "94/94 [==============================] - 73s 782ms/step - loss: 0.1700 - root_mean_squared_error: 0.4118 - val_loss: 0.3733 - val_root_mean_squared_error: 0.6110\n",
      "\n",
      "Epoch 00004: val_root_mean_squared_error did not improve from 0.50663\n",
      "Epoch 5/50\n",
      "94/94 [==============================] - 73s 781ms/step - loss: 0.1170 - root_mean_squared_error: 0.3418 - val_loss: 0.4981 - val_root_mean_squared_error: 0.7057\n",
      "\n",
      "Epoch 00005: val_root_mean_squared_error did not improve from 0.50663\n",
      "Epoch 6/50\n",
      "94/94 [==============================] - 73s 781ms/step - loss: 0.0735 - root_mean_squared_error: 0.2708 - val_loss: 0.3085 - val_root_mean_squared_error: 0.5554\n",
      "\n",
      "Epoch 00006: val_root_mean_squared_error did not improve from 0.50663\n",
      "Epoch 7/50\n",
      "94/94 [==============================] - 73s 782ms/step - loss: 0.0693 - root_mean_squared_error: 0.2629 - val_loss: 0.3477 - val_root_mean_squared_error: 0.5897\n",
      "\n",
      "Epoch 00007: val_root_mean_squared_error did not improve from 0.50663\n",
      "Epoch 8/50\n",
      "94/94 [==============================] - 73s 781ms/step - loss: 0.0586 - root_mean_squared_error: 0.2418 - val_loss: 0.5312 - val_root_mean_squared_error: 0.7288\n",
      "\n",
      "Epoch 00008: val_root_mean_squared_error did not improve from 0.50663\n",
      "Epoch 9/50\n",
      "94/94 [==============================] - 73s 782ms/step - loss: 0.0467 - root_mean_squared_error: 0.2159 - val_loss: 0.6270 - val_root_mean_squared_error: 0.7918\n",
      "\n",
      "Epoch 00009: val_root_mean_squared_error did not improve from 0.50663\n",
      "Epoch 10/50\n",
      "94/94 [==============================] - 74s 783ms/step - loss: 0.0635 - root_mean_squared_error: 0.2514 - val_loss: 0.5035 - val_root_mean_squared_error: 0.7096\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00010: val_root_mean_squared_error did not improve from 0.50663\n",
      "Epoch 00010: early stopping\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../input/huggingface-roberta/roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/huggingface-roberta/roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "94/94 [==============================] - 90s 816ms/step - loss: 1.1374 - root_mean_squared_error: 1.0548 - val_loss: 0.3416 - val_root_mean_squared_error: 0.5845\n",
      "\n",
      "Epoch 00001: val_root_mean_squared_error improved from inf to 0.58448, saving model to Roberta_Base_123_5.h5\n",
      "Epoch 2/50\n",
      "94/94 [==============================] - 74s 783ms/step - loss: 0.4085 - root_mean_squared_error: 0.6390 - val_loss: 0.3416 - val_root_mean_squared_error: 0.5844\n",
      "\n",
      "Epoch 00002: val_root_mean_squared_error improved from 0.58448 to 0.58445, saving model to Roberta_Base_123_5.h5\n",
      "Epoch 3/50\n",
      "94/94 [==============================] - 73s 781ms/step - loss: 0.3089 - root_mean_squared_error: 0.5555 - val_loss: 0.3347 - val_root_mean_squared_error: 0.5785\n",
      "\n",
      "Epoch 00003: val_root_mean_squared_error improved from 0.58445 to 0.57852, saving model to Roberta_Base_123_5.h5\n",
      "Epoch 4/50\n",
      "94/94 [==============================] - 73s 782ms/step - loss: 0.2105 - root_mean_squared_error: 0.4586 - val_loss: 0.3411 - val_root_mean_squared_error: 0.5840\n",
      "\n",
      "Epoch 00004: val_root_mean_squared_error did not improve from 0.57852\n",
      "Epoch 5/50\n",
      "94/94 [==============================] - 74s 783ms/step - loss: 0.1447 - root_mean_squared_error: 0.3798 - val_loss: 0.4161 - val_root_mean_squared_error: 0.6451\n",
      "\n",
      "Epoch 00005: val_root_mean_squared_error did not improve from 0.57852\n",
      "Epoch 6/50\n",
      "94/94 [==============================] - 73s 782ms/step - loss: 0.0927 - root_mean_squared_error: 0.3044 - val_loss: 0.4315 - val_root_mean_squared_error: 0.6569\n",
      "\n",
      "Epoch 00006: val_root_mean_squared_error did not improve from 0.57852\n",
      "Epoch 7/50\n",
      "94/94 [==============================] - 73s 781ms/step - loss: 0.0849 - root_mean_squared_error: 0.2913 - val_loss: 0.5678 - val_root_mean_squared_error: 0.7536\n",
      "\n",
      "Epoch 00007: val_root_mean_squared_error did not improve from 0.57852\n",
      "Epoch 8/50\n",
      "94/94 [==============================] - 74s 783ms/step - loss: 0.0795 - root_mean_squared_error: 0.2818 - val_loss: 0.3526 - val_root_mean_squared_error: 0.5938\n",
      "\n",
      "Epoch 00008: val_root_mean_squared_error did not improve from 0.57852\n",
      "Epoch 9/50\n",
      "94/94 [==============================] - 73s 782ms/step - loss: 0.0757 - root_mean_squared_error: 0.2748 - val_loss: 0.2470 - val_root_mean_squared_error: 0.4970\n",
      "\n",
      "Epoch 00009: val_root_mean_squared_error improved from 0.57852 to 0.49702, saving model to Roberta_Base_123_5.h5\n",
      "Epoch 10/50\n",
      "94/94 [==============================] - 73s 782ms/step - loss: 0.0653 - root_mean_squared_error: 0.2553 - val_loss: 0.4274 - val_root_mean_squared_error: 0.6537\n",
      "\n",
      "Epoch 00010: val_root_mean_squared_error did not improve from 0.49702\n",
      "Epoch 11/50\n",
      "94/94 [==============================] - 74s 783ms/step - loss: 0.0614 - root_mean_squared_error: 0.2476 - val_loss: 0.2895 - val_root_mean_squared_error: 0.5380\n",
      "\n",
      "Epoch 00011: val_root_mean_squared_error did not improve from 0.49702\n",
      "Epoch 12/50\n",
      "94/94 [==============================] - 74s 782ms/step - loss: 0.0614 - root_mean_squared_error: 0.2476 - val_loss: 0.4283 - val_root_mean_squared_error: 0.6544\n",
      "\n",
      "Epoch 00012: val_root_mean_squared_error did not improve from 0.49702\n",
      "Epoch 13/50\n",
      "94/94 [==============================] - 73s 781ms/step - loss: 0.0379 - root_mean_squared_error: 0.1945 - val_loss: 0.2711 - val_root_mean_squared_error: 0.5206\n",
      "\n",
      "Epoch 00013: val_root_mean_squared_error did not improve from 0.49702\n",
      "Epoch 14/50\n",
      "94/94 [==============================] - 73s 782ms/step - loss: 0.0376 - root_mean_squared_error: 0.1938 - val_loss: 0.2789 - val_root_mean_squared_error: 0.5281\n",
      "\n",
      "Epoch 00014: val_root_mean_squared_error did not improve from 0.49702\n",
      "Epoch 15/50\n",
      "94/94 [==============================] - 73s 781ms/step - loss: 0.0579 - root_mean_squared_error: 0.2399 - val_loss: 0.4102 - val_root_mean_squared_error: 0.6405\n",
      "\n",
      "Epoch 00015: val_root_mean_squared_error did not improve from 0.49702\n",
      "Epoch 16/50\n",
      "94/94 [==============================] - 74s 783ms/step - loss: 0.0342 - root_mean_squared_error: 0.1846 - val_loss: 0.3948 - val_root_mean_squared_error: 0.6283\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00016: val_root_mean_squared_error did not improve from 0.49702\n",
      "Epoch 00016: early stopping\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Our out of folds RMSE is 0.5284770649859224\n"
     ]
    }
   ],
   "source": [
    "# Function to build our model\n",
    "def build_roberta_base_model(max_len = MAX_LEN):\n",
    "    transformer = TFRobertaModel.from_pretrained(MODEL)\n",
    "    input_word_ids = tf.keras.layers.Input(shape = (max_len, ), dtype = tf.int32, name = 'input_word_ids')\n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "    # We only need the cls_token, resulting in a 2d array\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "#     x = tf.keras.layers.Dropout(0.09)(cls_token)\n",
    "    output = tf.keras.layers.Dense(1, activation = 'linear', dtype = 'float32')(cls_token)\n",
    "    model = tf.keras.models.Model(inputs = [input_word_ids], outputs = [output])\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(lr = LR),\n",
    "                  loss = [tf.keras.losses.MeanSquaredError()],\n",
    "                  metrics = [tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model\n",
    "\n",
    "# Function to train and evaluate our model\n",
    "test_predictions = []\n",
    "def train_and_evaluate():\n",
    "    \n",
    "    # Read our training data\n",
    "    df = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\n",
    "    df_test = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n",
    "#     df['excerpt'] = preprocess(df)\n",
    "#     df_test['excerpt'] = preprocess(df_test)\n",
    "    # Seed everything\n",
    "    seed_everything(SEED)\n",
    "    \n",
    "    # Initiate kfold object with shuffle and a specific seed\n",
    "    kfold = KFold(n_splits = FOLDS, shuffle = True, random_state = SEED)\n",
    "    # Create out of folds array to store predictions\n",
    "    oof_predictions = np.zeros(len(df))\n",
    "    \n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(df)):\n",
    "        print('\\n')\n",
    "        print('-'*50)\n",
    "        print(f'Training fold {fold + 1}')\n",
    "        K.clear_session()\n",
    "        # Get text features and target\n",
    "        x_train, x_val = df['excerpt'].iloc[trn_ind], df['excerpt'].iloc[val_ind]\n",
    "        y_train, y_val = df['target'].iloc[trn_ind].values, df['target'].iloc[val_ind].values\n",
    "        # Encode our text with Roberta tokenizer\n",
    "        x_train, x_val = encode_texts(x_train, x_val, MAX_LEN)\n",
    "        x_test = encode_texts_test(df_test['excerpt'], MAX_LEN)\n",
    "        # Function to transform our numpy array to a tf Dataset\n",
    "        train_dataset, valid_dataset = transform_to_tensors(x_train, x_val, y_train, y_val)\n",
    "        test_dataset = transform_to_tensors_test(x_test)\n",
    "        # Build model\n",
    "        model = build_roberta_base_model(max_len = MAX_LEN)\n",
    "        # Model checkpoint\n",
    "        es = tf.keras.callbacks.EarlyStopping(monitor='val_root_mean_squared_error', mode='min', \n",
    "                       patience=ES_PATIENCE, restore_best_weights=True, verbose=1)\n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(f'Roberta_Base_{SEED}_{fold + 1}.h5', \n",
    "                                                        monitor = 'val_root_mean_squared_error', \n",
    "                                                        verbose = VERBOSE, \n",
    "                                                        save_best_only = True,\n",
    "                                                        save_weights_only = True, \n",
    "                                                        mode = 'min')\n",
    "        steps = x_train.shape[0] // (BATCH_SIZE)\n",
    "        # Training phase\n",
    "        history = model.fit(train_dataset,\n",
    "                            batch_size = BATCH_SIZE,\n",
    "                            epochs = EPOCHS,\n",
    "                            verbose = VERBOSE,\n",
    "                            callbacks = [es,checkpoint],\n",
    "                            validation_data = valid_dataset,\n",
    "                            steps_per_epoch = steps)\n",
    "        \n",
    "        \n",
    "        # Load best epoch weights\n",
    "        model.load_weights(f'Roberta_Base_{SEED}_{fold + 1}.h5')\n",
    "        # Predict validation set to save them in the out of folds array\n",
    "        val_pred = model.predict(valid_dataset)\n",
    "        oof_predictions[val_ind] = val_pred.reshape(-1)\n",
    "        test_pred = model.predict(test_dataset)\n",
    "        test_predictions.append(test_pred)\n",
    "    print('\\n')\n",
    "    print('-'*50)\n",
    "    # Calculate out of folds root mean squared error\n",
    "    oof_rmse = np.sqrt(mean_squared_error(df['target'], oof_predictions))\n",
    "    print(f'Our out of folds RMSE is {oof_rmse}')\n",
    "    return oof_predictions,test_predictions\n",
    "    \n",
    "\n",
    "a,b=train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sudden-parcel",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T07:04:44.752167Z",
     "iopub.status.busy": "2021-06-10T07:04:44.751653Z",
     "iopub.status.idle": "2021-06-10T07:04:44.946367Z",
     "shell.execute_reply": "2021-06-10T07:04:44.947109Z",
     "shell.execute_reply.started": "2021-06-07T10:23:45.644864Z"
    },
    "papermill": {
     "duration": 1.690619,
     "end_time": "2021-06-10T07:04:44.947301",
     "exception": false,
     "start_time": "2021-06-10T07:04:43.256682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c0f722661</td>\n",
       "      <td>-0.197872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f0953f0a5</td>\n",
       "      <td>-0.239901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0df072751</td>\n",
       "      <td>-0.365530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04caf4e0c</td>\n",
       "      <td>-2.489808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0e63f8bea</td>\n",
       "      <td>-1.903589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12537fe78</td>\n",
       "      <td>-1.071037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>965e592c0</td>\n",
       "      <td>0.281837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    target\n",
       "0  c0f722661 -0.197872\n",
       "1  f0953f0a5 -0.239901\n",
       "2  0df072751 -0.365530\n",
       "3  04caf4e0c -2.489808\n",
       "4  0e63f8bea -1.903589\n",
       "5  12537fe78 -1.071037\n",
       "6  965e592c0  0.281837"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = pd.read_csv('/kaggle/input/commonlitreadabilityprize/test.csv')\n",
    "submission = test[['id']]\n",
    "submission['target'] = np.mean(b, axis=0)\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "display(submission.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-ceramic",
   "metadata": {
    "papermill": {
     "duration": 1.676759,
     "end_time": "2021-06-10T07:04:48.080540",
     "exception": false,
     "start_time": "2021-06-10T07:04:46.403781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-unemployment",
   "metadata": {
    "papermill": {
     "duration": 1.48687,
     "end_time": "2021-06-10T07:04:51.066276",
     "exception": false,
     "start_time": "2021-06-10T07:04:49.579406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4593.213722,
   "end_time": "2021-06-10T07:04:55.362896",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-10T05:48:22.149174",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
